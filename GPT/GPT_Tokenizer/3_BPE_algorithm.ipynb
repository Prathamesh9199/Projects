{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa59fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "You were There, When I need you the Most\n",
    "You are the One, I Knew, when I Saw you First.\n",
    "\n",
    "I still Remember, How we Met in the Mid of July.\n",
    "Since then, You became My Hardest Goodbye.\n",
    "Whenever I see you, I Fall for You, All over Again,\n",
    "My life was a Desert, you Came in like a Soothing Rain.\n",
    "\n",
    "You Always Showed me Path, Like a Flashlight,\n",
    "You are the One, The Moon of This Dark Night.\n",
    "\n",
    "They say, You came on Earth Alone, as a Half,\n",
    "You Wander in Search of Other, for a Complete Laugh.\n",
    "I knew, I found Mine, when I Get to know,\n",
    "We need No Words to Convey, What we Feel though.\n",
    "\n",
    "I was, I am, I'll wait in the Next Life too,\n",
    "Cause My Heart Knows, that it's Other Half is You.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2537397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary dictionary\n",
    "def create_vocab(text):\n",
    "\n",
    "    char_to_idx = {}\n",
    "    idx_to_char = {}\n",
    "\n",
    "    for idx, char in enumerate(list(set(text))):\n",
    "        unicode_code_point = char.encode('utf-8')\n",
    "\n",
    "        char_to_idx[unicode_code_point] = idx\n",
    "        idx_to_char[idx] = unicode_code_point\n",
    "\n",
    "    return char_to_idx, idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffdf0300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    global char_to_idx\n",
    "    \n",
    "    # encode text in utf-8 to get the list of unicode integers\n",
    "    byte_text = text.encode('utf-8')\n",
    "\n",
    "    return [char_to_idx[bytes([b])] for b in byte_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d294184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(unicode_code_point_list):\n",
    "    global idx_to_char\n",
    "\n",
    "    # decode list of unicode integers to raw text\n",
    "    byte_seq = b''.join([idx_to_char[idx] for idx in unicode_code_point_list])\n",
    "\n",
    "    return byte_seq.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe139f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens in raw text: 671\n",
      "vocab size would be: 48\n"
     ]
    }
   ],
   "source": [
    "# Raw unicode characters length\n",
    "char_to_idx, idx_to_char = create_vocab(text)\n",
    "encoded_unicode_chars = encode(text)\n",
    "print(f\"total tokens in raw text: {len(encoded_unicode_chars)}\")\n",
    "print(f\"vocab size would be: {len(set(encoded_unicode_chars))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ddfc1",
   "metadata": {},
   "source": [
    "```\n",
    "Assumptions:\n",
    "    1. LLM will have context size of 50.\n",
    "\n",
    "Initial observation:\n",
    "    1. The raw text has 671 tokens with 48 chars in the vocabulary.\n",
    "    2. With current context size of 50, we can only process 50 tokens at a time.\n",
    "    3. To process the entire text using a LLM with a context window of 50 tokens, we would need 14 segments (windows). (671/50 = 13.42, rounded up to 14).\n",
    "\n",
    "Goal:\n",
    "    1. We need an algorithm that reduces the total number of tokens by merging frequently occurring character or symbol pairs, so we can ideally process the entire text in fewer windows, or even in one window.\n",
    "    2. Making sure that the vocabulary size is not too large, as the vocab size determines the memory required to store token embeddings and increases model complexity. A large vocabulary can lead to sparsity and make the model harder to train.\n",
    "\n",
    "Note: \n",
    "    1. There is a trade-off between the number of tokens and the vocabulary size, larger vocab size means more tokens can be represented, but it also increases the model complexity and memory requirements.\n",
    "    2. Also larger vocab means more scarcity, as in the end steps of transformer we have to predict the next token out of our vocabulary, i.e. the overall probability distribution of the next token is over the vocabulary size.\n",
    "    3. So, we need to find a balance between the number of tokens and the vocabulary size.\n",
    "    4. The goal is to reduce the number of tokens while keeping the vocabulary size manageable.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supporting functions for bpe\n",
    "def get_stats(unicode_code_point_list: list) -> dict:\n",
    "    \"\"\"    \n",
    "        Get the frequency of each pair of consecutive bytes in the byte list.  \n",
    "\n",
    "        Sample usage:\n",
    "        >>> get_stats([1, 2, 3, 4, 1, 2, 4, 5, 6, 3, 4, 2, 3, 4])\n",
    "        {\n",
    "            (3, 4): 3,\n",
    "            (1, 2): 2,\n",
    "            (2, 3): 2,\n",
    "            (4, 1): 1,\n",
    "            (2, 4): 1,\n",
    "            (4, 5): 1,\n",
    "            (5, 6): 1,\n",
    "            (6, 3): 1,\n",
    "            (4, 2): 1\n",
    "            }\n",
    "\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    for itm1, itm2 in zip(unicode_code_point_list, unicode_code_point_list[1:]):\n",
    "        pair = (itm1, itm2)\n",
    "\n",
    "        stats[pair] = stats.get(pair, 0) + 1\n",
    "\n",
    "    #return the stats dictionary as a dictionary but sorted in descending order of values\n",
    "    return {k: v for k, v in sorted(stats.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761ac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting vocab size: 48\n",
      "Final vocab size: 135\n",
      "total tokens in raw text: 671\n",
      "total tokens after bpe: 312\n",
      "compression ratio: 46.497764530551414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = \"Hey there, how are you doing today? I hope you're having a great day! Let's make the most of it together.\"\n",
    "char_to_idx, idx_to_char = create_vocab(text)\n",
    "\n",
    "def merge(unicode_code_point_list: list) -> list:\n",
    "    global char_to_idx, idx_to_char\n",
    "\n",
    "    # get the dictionary of all the pairs and their occurrences\n",
    "    stats = get_stats(unicode_code_point_list)\n",
    "\n",
    "    # pair with max occurrences\n",
    "    try:\n",
    "        pair_with_max_occurrences = max(stats, key=stats.get)\n",
    "    except ValueError:\n",
    "        # if stats is empty, return the original unicode_code_point_list\n",
    "        return unicode_code_point_list\n",
    "    \n",
    "    if stats[pair_with_max_occurrences] > 1:\n",
    "\n",
    "        # mint a new token for the pair & new idx\n",
    "        new_token = idx_to_char[pair_with_max_occurrences[0]] + idx_to_char[pair_with_max_occurrences[1]]\n",
    "        new_token_idx = len(idx_to_char)\n",
    "        # print(f\"new token: {new_token}, new token idx: {new_token_idx}\")\n",
    "\n",
    "        # add the new token to the char_to_idx and idx_to_char dictionaries\n",
    "        char_to_idx[new_token] = new_token_idx\n",
    "        idx_to_char[new_token_idx] = new_token\n",
    "\n",
    "        # replace those pairs in the byte list with the new token\n",
    "        i = 0\n",
    "        while i < len(unicode_code_point_list) - 1:\n",
    "            if (unicode_code_point_list[i], unicode_code_point_list[i+1]) == pair_with_max_occurrences:\n",
    "                unicode_code_point_list[i] = new_token_idx\n",
    "                unicode_code_point_list.pop(i + 1) \n",
    "            else:\n",
    "                i += 1           \n",
    "    else:\n",
    "        # if no pairs found, return the original unicode_code_point_list\n",
    "        return unicode_code_point_list, False\n",
    "        \n",
    "    return unicode_code_point_list, True\n",
    "\n",
    "def bpe(text):\n",
    "    global char_to_idx, idx_to_char\n",
    "\n",
    "    starting_vocab_size = len(idx_to_char)\n",
    "\n",
    "    # encode text in utf-8 to get the list of unicode integers\n",
    "    unicode_code_point_list = encode(text)\n",
    "    raw_text_token_count = len(unicode_code_point_list)\n",
    "\n",
    "    flag = True\n",
    "    while flag:\n",
    "        # continue merging until no more pairs can be merged\n",
    "        unicode_code_point_list, flag = merge(unicode_code_point_list)\n",
    "\n",
    "    processed_text_token_count = len(unicode_code_point_list)\n",
    "    compression_ratio = (processed_text_token_count / raw_text_token_count) * 100\n",
    "    final_vocab_size = len(idx_to_char)\n",
    "\n",
    "    print(f\"Starting vocab size: {starting_vocab_size}\")\n",
    "    print(f\"Final vocab size: {final_vocab_size}\")\n",
    "    print(f\"total tokens in raw text: {raw_text_token_count}\")\n",
    "    print(f\"total tokens after bpe: {processed_text_token_count}\")\n",
    "    print(f\"compression ratio: {compression_ratio}\")\n",
    "    \n",
    "    return unicode_code_point_list    \n",
    "\n",
    "decode(bpe(text)) == text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
