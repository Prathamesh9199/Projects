{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482ef101",
   "metadata": {},
   "source": [
    "#### Goal here is to convert text (from english and other langugages, symbols & special characters) in numbers so we can plug it into transformer architecture\n",
    "\n",
    "Tokenizer app [https://tiktokenizer.vercel.app/]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e63350",
   "metadata": {},
   "source": [
    "in python strings are stored as unicode characters\n",
    "for example: \n",
    "\n",
    "```string = \"Hello World! ðŸ˜Š, à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚ à¤†à¤ª\"```\n",
    "\n",
    "we can get the unicode representation of each character in the string using the ord() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1196ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character: H - Unicode: 72\n",
      "Character: e - Unicode: 101\n",
      "Character: l - Unicode: 108\n",
      "Character: l - Unicode: 108\n",
      "Character: o - Unicode: 111\n",
      "Character:   - Unicode: 32\n",
      "Character: W - Unicode: 87\n",
      "Character: o - Unicode: 111\n",
      "Character: r - Unicode: 114\n",
      "Character: l - Unicode: 108\n",
      "Character: d - Unicode: 100\n",
      "Character: ! - Unicode: 33\n",
      "Character:   - Unicode: 32\n",
      "Character: ðŸ˜Š - Unicode: 128522\n",
      "Character: , - Unicode: 44\n",
      "Character:   - Unicode: 32\n",
      "Character: à¤• - Unicode: 2325\n",
      "Character: à¥ˆ - Unicode: 2376\n",
      "Character: à¤¸ - Unicode: 2360\n",
      "Character: à¥‡ - Unicode: 2375\n",
      "Character:   - Unicode: 32\n",
      "Character: à¤¹ - Unicode: 2361\n",
      "Character: à¥ˆ - Unicode: 2376\n",
      "Character: à¤‚ - Unicode: 2306\n",
      "Character:   - Unicode: 32\n",
      "Character: à¤† - Unicode: 2310\n",
      "Character: à¤ª - Unicode: 2346\n"
     ]
    }
   ],
   "source": [
    "string = \"Hello World! ðŸ˜Š, à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚ à¤†à¤ª\"\n",
    "\n",
    "unicode_representation = [ord(char) for char in string]\n",
    "for char, unicode in zip(string, unicode_representation):\n",
    "    print(f\"Character: {char} - Unicode: {unicode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569a30a7",
   "metadata": {},
   "source": [
    "**Question is**, why can't we use directly these unicode characters directly as tokens, as they are already numbers, and we can map each number to a text, symbol, special charharacter or emoji?\n",
    "\n",
    "Answer -\n",
    "1. It will increase the vocabulary size of our model. [Unicode characters is a list of 292k unicode codepoints, https://en.wikipedia.org/wiki/List_of_Unicode_characters]\n",
    "2. Also unicode standard is alive and keep changing which makes it difficult maintain a standard vocabulary for our model\n",
    "\n",
    "\n",
    "Another way is to do UTF encoding (utf-8, utf-16, utf-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574a356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UTF-8 Encoded Bytes length:  48\n",
      "UTF-16 Encoded Bytes length:  58\n",
      "UTF-32 Encoded Bytes length:  112\n"
     ]
    }
   ],
   "source": [
    "utf_8_lst = list(string.encode('utf-8'))\n",
    "utf_16_lst = list(string.encode('utf-16'))\n",
    "utf_32_lst = list(string.encode('utf-32'))\n",
    "\n",
    "print(\"\\nUTF-8 Encoded Bytes length: \", len(utf_8_lst))\n",
    "print(\"UTF-16 Encoded Bytes length: \", len(utf_16_lst))\n",
    "print(\"UTF-32 Encoded Bytes length: \", len(utf_32_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b002c",
   "metadata": {},
   "source": [
    "Here we can start seeing that utf-16 & 32 are wasteful for our purpose, cause they introduce lots of 0's in between due to nature of UTF encoding (utf-8 is byte level encoding i.e 1 byte, utf-16 is 2 bytes, utf-32 is 3 bytes)\n",
    "\n",
    "also, if we use the utf-8 encoder, we are stuck with 1 byte, i.e. 8 chars that is max 256 tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e07936",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding Algorithm\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Byte-pair_encoding]\n",
    "\n",
    "Example <br>\n",
    "Suppose the data to be encoded is: aaabdaaabac <br><br>\n",
    "\n",
    "The byte pair \"aa\" occurs most often, so it will be replaced by a byte that is not used in the data, such as \"Z\". Now there is the following data and replacement table:<br><br>\n",
    "\n",
    "ZabdZabac<br>\n",
    "Z=aa<br>\n",
    "Then the process is repeated with byte pair \"ab\", replacing it with \"Y\":<br><br>\n",
    "\n",
    "ZYdZYac<br>\n",
    "Y=ab<br>\n",
    "Z=aa<br>\n",
    "The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte-pair encoding, replacing \"ZY\" with \"X\":<br><br>\n",
    "\n",
    "XdXac<br>\n",
    "X=ZY<br>\n",
    "Y=ab<br>\n",
    "Z=aa<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0274934c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c19ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
